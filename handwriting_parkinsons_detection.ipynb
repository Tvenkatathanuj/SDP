{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86df3df1",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q efficientnet-pytorch albumentations grad-cam scikit-plot\n",
    "!pip install -q timm torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "# Augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f62c25c",
   "metadata": {},
   "source": [
    "## 2. Clone Dataset from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fade6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/Tvenkatathanuj/SDP.git\n",
    "\n",
    "# Set paths\n",
    "DATA_PATH = '/content/SDP/handwritten dataset/Dataset/Dataset'\n",
    "HEALTHY_PATH = os.path.join(DATA_PATH, 'Healthy')\n",
    "PARKINSON_PATH = os.path.join(DATA_PATH, 'Parkinson')\n",
    "\n",
    "print(f\"Healthy samples: {len(os.listdir(HEALTHY_PATH))}\")\n",
    "print(f\"Parkinson samples: {len(os.listdir(PARKINSON_PATH))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9575dc3b",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset dataframe\n",
    "data = []\n",
    "\n",
    "for img_name in os.listdir(HEALTHY_PATH):\n",
    "    data.append({\n",
    "        'image_path': os.path.join(HEALTHY_PATH, img_name),\n",
    "        'label': 0,  # Healthy\n",
    "        'class_name': 'Healthy'\n",
    "    })\n",
    "\n",
    "for img_name in os.listdir(PARKINSON_PATH):\n",
    "    data.append({\n",
    "        'image_path': os.path.join(PARKINSON_PATH, img_name),\n",
    "        'label': 1,  # Parkinson\n",
    "        'class_name': 'Parkinson'\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nClass distribution:\\n{df['class_name'].value_counts()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3292cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Sample Handwriting Images', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Healthy samples\n",
    "healthy_samples = df[df['label'] == 0].sample(5)\n",
    "for i, (idx, row) in enumerate(healthy_samples.iterrows()):\n",
    "    img = Image.open(row['image_path'])\n",
    "    axes[0, i].imshow(img, cmap='gray')\n",
    "    axes[0, i].set_title('Healthy', color='green', fontweight='bold')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Parkinson samples\n",
    "parkinson_samples = df[df['label'] == 1].sample(5)\n",
    "for i, (idx, row) in enumerate(parkinson_samples.iterrows()):\n",
    "    img = Image.open(row['image_path'])\n",
    "    axes[1, i].imshow(img, cmap='gray')\n",
    "    axes[1, i].set_title('Parkinson', color='red', fontweight='bold')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797ee1d",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset with Advanced Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, use_augmentation=False):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.use_augmentation = use_augmentation\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['image_path']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Define augmentations\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(380, 380),\n",
    "    A.RandomCrop(336, 336),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=20, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0)),\n",
    "        A.GaussianBlur(blur_limit=(3, 7)),\n",
    "        A.MotionBlur(blur_limit=5),\n",
    "    ], p=0.3),\n",
    "    A.OneOf([\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3),\n",
    "        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50),\n",
    "    ], p=0.3),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(336, 336),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "print(\"Augmentation pipelines created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4203f4",
   "metadata": {},
   "source": [
    "## 5. Novel Model Architecture: EfficientNet-B4 + SPP + CBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c42bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"CBAM Channel Attention Module\"\"\"\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"CBAM Spatial Attention Module\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_attention(x)\n",
    "        x = x * self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "class SpatialPyramidPooling(nn.Module):\n",
    "    \"\"\"Spatial Pyramid Pooling for multi-scale features\"\"\"\n",
    "    def __init__(self, pool_sizes=[1, 2, 4]):\n",
    "        super(SpatialPyramidPooling, self).__init__()\n",
    "        self.pool_sizes = pool_sizes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, _, _ = x.size()\n",
    "        pools = []\n",
    "        \n",
    "        for pool_size in self.pool_sizes:\n",
    "            pool = F.adaptive_avg_pool2d(x, (pool_size, pool_size))\n",
    "            pool = pool.view(batch_size, -1)  # Flatten to (batch, channels * pool_size^2)\n",
    "            pools.append(pool)\n",
    "        \n",
    "        out = torch.cat(pools, dim=1)  # Concatenate along feature dimension\n",
    "        return out\n",
    "\n",
    "class HandwritingParkinsonsModel(nn.Module):\n",
    "    \"\"\"Novel Architecture: EfficientNet-B4 + SPP + CBAM + Multi-Head Classifier\"\"\"\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super(HandwritingParkinsonsModel, self).__init__()\n",
    "        \n",
    "        # EfficientNet-B4 backbone\n",
    "        self.backbone = timm.create_model('efficientnet_b4', pretrained=pretrained, features_only=True)\n",
    "        \n",
    "        # Get actual output channels from the last feature map\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            features = self.backbone(dummy_input)\n",
    "            feature_dim = features[-1].shape[1]  # Get channels from last feature map\n",
    "        \n",
    "        # CBAM attention\n",
    "        self.cbam = CBAM(feature_dim)\n",
    "        \n",
    "        # Spatial Pyramid Pooling\n",
    "        self.spp = SpatialPyramidPooling(pool_sizes=[1, 2, 4])\n",
    "        spp_out_dim = feature_dim * (1*1 + 2*2 + 4*4)  # feature_dim * 21\n",
    "        \n",
    "        # Classification head with stronger regularization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(spp_out_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Feature extractor for fusion model\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(spp_out_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_features=False):\n",
    "        # Extract features from backbone\n",
    "        features = self.backbone(x)\n",
    "        x = features[-1]  # Get last feature map\n",
    "        \n",
    "        # Apply CBAM attention\n",
    "        x = self.cbam(x)\n",
    "        \n",
    "        # Spatial Pyramid Pooling\n",
    "        x = self.spp(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        if return_features:\n",
    "            # Return features for fusion\n",
    "            features = self.feature_extractor(x)\n",
    "            return features\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(x)\n",
    "        return out\n",
    "\n",
    "print(\"Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152f1e1",
   "metadata": {},
   "source": [
    "## 6. Focal Loss for Imbalanced Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling hard examples\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "print(\"Focal Loss defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b232c6d",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Mixup data augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup loss criterion\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, scaler=None, use_mixup=False):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Apply Mixup\n",
    "        if use_mixup:\n",
    "            images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                if use_mixup:\n",
    "                    loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "                else:\n",
    "                    loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            if use_mixup:\n",
    "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        if use_mixup:\n",
    "            all_labels.extend(labels_a.cpu().numpy())  # Use original labels for accuracy\n",
    "        else:\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': running_loss / (pbar.n + 1)})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds) if all_labels else 0\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': running_loss / (pbar.n + 1)})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_labels, all_preds, all_probs\n",
    "\n",
    "print(\"Training functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed7fb4",
   "metadata": {},
   "source": [
    "## 8. Data Preparation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235be6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HandwritingDataset(train_df, transform=train_transform, use_augmentation=True)\n",
    "val_dataset = HandwritingDataset(val_df, transform=val_transform, use_augmentation=False)\n",
    "test_dataset = HandwritingDataset(test_df, transform=val_transform, use_augmentation=False)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"DataLoaders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = HandwritingParkinsonsModel(num_classes=2, pretrained=True).to(device)\n",
    "\n",
    "# Loss and optimizer with stronger regularization\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=5e-4)  # Lower LR, higher weight decay\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"Ready to train!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713781f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "NUM_EPOCHS = 50\n",
    "best_val_acc = 0.0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, scaler)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, _, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model based on validation loss (not accuracy to prevent overfitting)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "        }, 'best_handwriting_model.pth')\n",
    "\n",
    "        print(f\"✓ Model saved with val_acc: {val_acc:.4f}, val_loss: {val_loss:.4f}\")print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    else:print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "        patience_counter += 1\n",
    "\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{early_stopping_patience}\")        break\n",
    "\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "\n",
    "    # Early stopping    if patience_counter >= early_stopping_patience:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdc1d8",
   "metadata": {},
   "source": [
    "## 9. Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e50a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_handwriting_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Test evaluation\n",
    "test_loss, test_acc, y_true, y_pred, y_probs = validate_epoch(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TEST SET PERFORMANCE\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Healthy', 'Parkinson'], digits=4))\n",
    "\n",
    "# Additional metrics\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_probs)\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('handwriting_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acdd0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and ROC Curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Healthy', 'Parkinson'],\n",
    "            yticklabels=['Healthy', 'Parkinson'], ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title(f'Confusion Matrix\\nAccuracy: {test_acc:.4f}')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "axes[1].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})', linewidth=2)\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('handwriting_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6430a1e0",
   "metadata": {},
   "source": [
    "## 10. Save Model for Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'test_acc': test_acc,\n",
    "    'test_auc': auc,\n",
    "}, 'handwriting_parkinsons_model_final.pth')\n",
    "\n",
    "print(\"✓ Model saved for fusion: handwriting_parkinsons_model_final.pth\")\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
