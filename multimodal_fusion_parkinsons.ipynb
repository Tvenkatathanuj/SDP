{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459d57a9",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d57852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision timm transformers\n",
    "!pip install -q librosa soundfile albumentations\n",
    "!pip install -q scikit-learn matplotlib seaborn tqdm\n",
    "!pip install -q efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe253fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Set seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57587405",
   "metadata": {},
   "source": [
    "## 2. Clone Dataset and Load Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15387dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/Tvenkatathanuj/SDP.git\n",
    "\n",
    "# Paths\n",
    "HANDWRITING_PATH = '/content/SDP/handwritten dataset/Dataset/Dataset'\n",
    "SPEECH_PATH = '/content/SDP/speech dataset'\n",
    "\n",
    "print(\"Dataset cloned successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained models from GitHub repository\n",
    "print(\"Downloading pre-trained models from GitHub...\")\n",
    "\n",
    "# These models should be trained first using the individual notebooks\n",
    "# The model files will be in the root of the repository after training\n",
    "\n",
    "# For now, we'll use placeholder paths - you need to train individual models first\n",
    "print(\"\\n⚠️ IMPORTANT: Before running fusion model, you must:\")\n",
    "print(\"1. Train handwriting model (handwriting_parkinsons_detection.ipynb)\")\n",
    "print(\"2. Train speech model (speech_parkinsons_detection.ipynb)\")\n",
    "print(\"3. Download the .pth files from Colab\")\n",
    "print(\"4. Upload them to this Colab session or push to GitHub\")\n",
    "print(\"\\nOnce models are trained, the .pth files should be in the same directory.\")\n",
    "print(\"✓ Ready to load models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a0a9d",
   "metadata": {},
   "source": [
    "## 3. Define Individual Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0569d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy handwriting model architecture\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.spatial_conv = nn.Conv2d(2, 1, 7, padding=3, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        channel_att = self.sigmoid(avg_out + max_out)\n",
    "        x = x * channel_att\n",
    "        \n",
    "        # Spatial attention\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial_att = self.sigmoid(self.spatial_conv(torch.cat([avg_out, max_out], dim=1)))\n",
    "        x = x * spatial_att\n",
    "        return x\n",
    "\n",
    "class SpatialPyramidPooling(nn.Module):\n",
    "    def __init__(self, pool_sizes=[1, 2, 4]):\n",
    "        super(SpatialPyramidPooling, self).__init__()\n",
    "        self.pool_sizes = pool_sizes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, _, _ = x.size()\n",
    "        pools = []\n",
    "        for pool_size in self.pool_sizes:\n",
    "            pool = F.adaptive_avg_pool2d(x, (pool_size, pool_size))\n",
    "            pool = pool.view(batch_size, channels, -1)\n",
    "            pools.append(pool)\n",
    "        return torch.cat(pools, dim=2)\n",
    "\n",
    "class HandwritingModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(HandwritingModel, self).__init__()\n",
    "        self.backbone = timm.create_model('efficientnet_b4', pretrained=False, features_only=True)\n",
    "        \n",
    "        # Get actual output channels dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            features = self.backbone(dummy_input)\n",
    "            feature_dim = features[-1].shape[1]\n",
    "        \n",
    "        self.cbam = CBAM(feature_dim)\n",
    "        self.spp = SpatialPyramidPooling()\n",
    "        spp_dim = feature_dim * 21  # Dynamic calculation\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(spp_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Updated to match improved model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(spp_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_features=False):\n",
    "        features = self.backbone(x)\n",
    "        x = features[-1]\n",
    "        x = self.cbam(x)\n",
    "        x = self.spp(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        if return_features:\n",
    "            return self.feature_extractor(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "print(\"Handwriting model architecture loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2deb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy speech model architecture\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attended, _ = self.attention(x, x, x)\n",
    "        return self.norm(x + self.dropout(attended))\n",
    "\n",
    "class SpeechModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, acoustic_feature_dim=110):\n",
    "        super(SpeechModel, self).__init__()\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        \n",
    "        self.lstm = nn.LSTM(768, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        self.attention = MultiHeadAttention(512, num_heads=8)\n",
    "        \n",
    "        # Updated deeper acoustic branch\n",
    "        self.acoustic_branch = nn.Sequential(\n",
    "            nn.Linear(acoustic_feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        fusion_dim = 512 + 128\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_values, acoustic_features, return_features=False):\n",
    "        wav2vec_out = self.wav2vec(input_values).last_hidden_state\n",
    "        lstm_out, _ = self.lstm(wav2vec_out)\n",
    "        attended = self.attention(lstm_out)\n",
    "        pooled = torch.mean(attended, dim=1)\n",
    "        acoustic_out = self.acoustic_branch(acoustic_features)\n",
    "        fused = torch.cat([pooled, acoustic_out], dim=1)\n",
    "        \n",
    "        if return_features:\n",
    "            return self.feature_extractor(fused)\n",
    "        return self.classifier(fused)\n",
    "\n",
    "print(\"Speech model architecture loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ff27e",
   "metadata": {},
   "source": [
    "## 4. Novel Fusion Architecture: Cross-Modal Attention Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25df3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"Cross-modal attention for handwriting-speech feature fusion\"\"\"\n",
    "    def __init__(self, dim1, dim2, hidden_dim=256):\n",
    "        super(CrossModalAttention, self).__init__()\n",
    "        \n",
    "        # Project features to common space\n",
    "        self.proj1 = nn.Linear(dim1, hidden_dim)\n",
    "        self.proj2 = nn.Linear(dim2, hidden_dim)\n",
    "        \n",
    "        # Cross-attention\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.scale = hidden_dim ** -0.5\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, feat1, feat2):\n",
    "        # Project to common space\n",
    "        f1 = self.proj1(feat1)  # (B, hidden_dim)\n",
    "        f2 = self.proj2(feat2)  # (B, hidden_dim)\n",
    "        \n",
    "        # Cross-attention: feat1 attends to feat2\n",
    "        q = self.query(f1).unsqueeze(1)  # (B, 1, hidden_dim)\n",
    "        k = self.key(f2).unsqueeze(1)    # (B, 1, hidden_dim)\n",
    "        v = self.value(f2).unsqueeze(1)  # (B, 1, hidden_dim)\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        attended_f2 = torch.matmul(attn, v).squeeze(1)\n",
    "        \n",
    "        # Reverse: feat2 attends to feat1\n",
    "        q2 = self.query(f2).unsqueeze(1)\n",
    "        k2 = self.key(f1).unsqueeze(1)\n",
    "        v2 = self.value(f1).unsqueeze(1)\n",
    "        \n",
    "        attn2 = torch.matmul(q2, k2.transpose(-2, -1)) * self.scale\n",
    "        attn2 = F.softmax(attn2, dim=-1)\n",
    "        attn2 = self.dropout(attn2)\n",
    "        \n",
    "        attended_f1 = torch.matmul(attn2, v2).squeeze(1)\n",
    "        \n",
    "        return attended_f1, attended_f2\n",
    "\n",
    "class UncertaintyModule(nn.Module):\n",
    "    \"\"\"Uncertainty quantification using Monte Carlo Dropout\"\"\"\n",
    "    def __init__(self, input_dim, dropout_rate=0.3):\n",
    "        super(UncertaintyModule, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(input_dim, 2)  # Confidence scores\n",
    "    \n",
    "    def forward(self, x, n_samples=10):\n",
    "        # Monte Carlo Dropout\n",
    "        predictions = []\n",
    "        for _ in range(n_samples):\n",
    "            dropped = self.dropout(x)\n",
    "            pred = self.fc(dropped)\n",
    "            predictions.append(F.softmax(pred, dim=1))\n",
    "        \n",
    "        # Mean and variance\n",
    "        predictions = torch.stack(predictions)\n",
    "        mean_pred = predictions.mean(dim=0)\n",
    "        uncertainty = predictions.var(dim=0).mean(dim=1)  # Average variance across classes\n",
    "        \n",
    "        return mean_pred, uncertainty\n",
    "\n",
    "class MultimodalFusionModel(nn.Module):\n",
    "    \"\"\"Novel Cross-Modal Attention Fusion with Uncertainty Quantification\"\"\"\n",
    "    def __init__(self, handwriting_model, speech_model, num_classes=2):\n",
    "        super(MultimodalFusionModel, self).__init__()\n",
    "        \n",
    "        self.handwriting_model = handwriting_model\n",
    "        self.speech_model = speech_model\n",
    "        \n",
    "        # Freeze individual models initially\n",
    "        for param in self.handwriting_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.speech_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.cross_attention = CrossModalAttention(dim1=512, dim2=512, hidden_dim=256)\n",
    "        \n",
    "        # Uncertainty modules\n",
    "        self.uncertainty_hand = UncertaintyModule(256)\n",
    "        self.uncertainty_speech = UncertaintyModule(256)\n",
    "        \n",
    "        # Adaptive fusion weights\n",
    "        self.fusion_weights = nn.Parameter(torch.tensor([0.5, 0.5]))\n",
    "        \n",
    "        # Final fusion classifier\n",
    "        fusion_dim = 256 * 2  # Two modalities\n",
    "        self.fusion_classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Ensemble predictor\n",
    "        self.ensemble = nn.Linear(num_classes * 3, num_classes)  # 3 predictions\n",
    "    \n",
    "    def forward(self, image=None, audio=None, acoustic_feat=None, mode='fusion'):\n",
    "        if mode == 'handwriting_only':\n",
    "            return self.handwriting_model(image)\n",
    "        \n",
    "        elif mode == 'speech_only':\n",
    "            return self.speech_model(audio, acoustic_feat)\n",
    "        \n",
    "        elif mode == 'fusion':\n",
    "            # Extract features from both modalities\n",
    "            hand_features = self.handwriting_model(image, return_features=True)\n",
    "            speech_features = self.speech_model(audio, acoustic_feat, return_features=True)\n",
    "            \n",
    "            # Cross-modal attention\n",
    "            attended_hand, attended_speech = self.cross_attention(hand_features, speech_features)\n",
    "            \n",
    "            # Get predictions with uncertainty\n",
    "            hand_pred, hand_uncertainty = self.uncertainty_hand(attended_hand)\n",
    "            speech_pred, speech_uncertainty = self.uncertainty_speech(attended_speech)\n",
    "            \n",
    "            # Adaptive weighting based on uncertainty (lower uncertainty = higher weight)\n",
    "            hand_confidence = 1.0 / (1.0 + hand_uncertainty)\n",
    "            speech_confidence = 1.0 / (1.0 + speech_uncertainty)\n",
    "            \n",
    "            total_confidence = hand_confidence + speech_confidence\n",
    "            hand_weight = hand_confidence / total_confidence\n",
    "            speech_weight = speech_confidence / total_confidence\n",
    "            \n",
    "            # Weighted predictions\n",
    "            weighted_pred = hand_weight.unsqueeze(1) * hand_pred + speech_weight.unsqueeze(1) * speech_pred\n",
    "            \n",
    "            # Fusion features\n",
    "            fused_features = torch.cat([attended_hand, attended_speech], dim=1)\n",
    "            fusion_pred = self.fusion_classifier(fused_features)\n",
    "            \n",
    "            # Ensemble\n",
    "            ensemble_input = torch.cat([hand_pred, speech_pred, F.softmax(fusion_pred, dim=1)], dim=1)\n",
    "            final_pred = self.ensemble(ensemble_input)\n",
    "            \n",
    "            return final_pred, {\n",
    "                'hand_pred': hand_pred,\n",
    "                'speech_pred': speech_pred,\n",
    "                'fusion_pred': fusion_pred,\n",
    "                'weighted_pred': weighted_pred,\n",
    "                'hand_uncertainty': hand_uncertainty,\n",
    "                'speech_uncertainty': speech_uncertainty,\n",
    "                'hand_weight': hand_weight,\n",
    "                'speech_weight': speech_weight\n",
    "            }\n",
    "\n",
    "print(\"✓ Multimodal Fusion Model architecture created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5c42f",
   "metadata": {},
   "source": [
    "## 5. Multimodal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_acoustic_features(audio_path, sr=16000):\n",
    "    \"\"\"Extract acoustic features (110 dimensions)\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=sr)\n",
    "        \n",
    "        # MFCCs (40 mean + 40 std = 80)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "        mfcc_mean = np.mean(mfccs, axis=1)\n",
    "        mfcc_std = np.std(mfccs, axis=1)\n",
    "        \n",
    "        # Chroma (12 mean + 12 std = 24)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=12)\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "        chroma_std = np.std(chroma, axis=1)\n",
    "        \n",
    "        # Spectral features (4)\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "        spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "        \n",
    "        # Jitter and Shimmer (2)\n",
    "        try:\n",
    "            sound = parselmouth.Sound(audio_path)\n",
    "            pitch = call(sound, \"To Pitch\", 0.0, 75, 600)\n",
    "            point_process = call(sound, \"To PointProcess (periodic, cc)\", 75, 600)\n",
    "            jitter = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "            shimmer = call([sound, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "        except:\n",
    "            jitter, shimmer = 0, 0\n",
    "        \n",
    "        # Total: 80 + 24 + 4 + 2 = 110\n",
    "        feature_vector = np.concatenate([\n",
    "            mfcc_mean, mfcc_std,\n",
    "            chroma_mean, chroma_std,\n",
    "            [spectral_centroid, spectral_rolloff, spectral_bandwidth, zcr, jitter, shimmer]\n",
    "        ])\n",
    "        return feature_vector\n",
    "    except:\n",
    "        return np.zeros(110)\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, handwriting_df, speech_df, processor, image_transform, max_audio_length=80000):\n",
    "        # Match samples by label (since we have different counts)\n",
    "        self.samples = []\n",
    "        \n",
    "        # For each speech sample, pair with random handwriting sample of same class\n",
    "        for _, speech_row in speech_df.iterrows():\n",
    "            label = speech_row['label']\n",
    "            hand_sample = handwriting_df[handwriting_df['label'] == label].sample(1).iloc[0]\n",
    "            \n",
    "            self.samples.append({\n",
    "                'image_path': hand_sample['image_path'],\n",
    "                'audio_path': speech_row['audio_path'],\n",
    "                'label': label\n",
    "            })\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.image_transform = image_transform\n",
    "        self.max_audio_length = max_audio_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load and process image\n",
    "        image = cv2.imread(sample['image_path'])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.image_transform(image=image)['image']\n",
    "        \n",
    "        # Load and process audio\n",
    "        y, sr = librosa.load(sample['audio_path'], sr=16000)\n",
    "        if len(y) < self.max_audio_length:\n",
    "            y = np.pad(y, (0, self.max_audio_length - len(y)))\n",
    "        else:\n",
    "            y = y[:self.max_audio_length]\n",
    "        \n",
    "        audio_input = self.processor(y, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Extract acoustic features\n",
    "        acoustic_features = extract_acoustic_features(sample['audio_path'])\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'audio': audio_input.input_values.squeeze(0),\n",
    "            'acoustic_features': torch.FloatTensor(acoustic_features),\n",
    "            'label': torch.LongTensor([sample['label']])[0]\n",
    "        }\n",
    "\n",
    "print(\"Multimodal dataset class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a61030",
   "metadata": {},
   "source": [
    "## 6. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eec812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare handwriting data\n",
    "hand_data = []\n",
    "for img in os.listdir(os.path.join(HANDWRITING_PATH, 'Healthy')):\n",
    "    hand_data.append({'image_path': os.path.join(HANDWRITING_PATH, 'Healthy', img), 'label': 0})\n",
    "for img in os.listdir(os.path.join(HANDWRITING_PATH, 'Parkinson')):\n",
    "    hand_data.append({'image_path': os.path.join(HANDWRITING_PATH, 'Parkinson', img), 'label': 1})\n",
    "hand_df = pd.DataFrame(hand_data)\n",
    "\n",
    "# Prepare speech data\n",
    "speech_data = []\n",
    "for audio in os.listdir(os.path.join(SPEECH_PATH, 'HC_AH/HC_AH')):\n",
    "    if audio.endswith('.wav'):\n",
    "        speech_data.append({'audio_path': os.path.join(SPEECH_PATH, 'HC_AH/HC_AH', audio), 'label': 0})\n",
    "for audio in os.listdir(os.path.join(SPEECH_PATH, 'PD_AH/PD_AH')):\n",
    "    if audio.endswith('.wav'):\n",
    "        speech_data.append({'audio_path': os.path.join(SPEECH_PATH, 'PD_AH/PD_AH', audio), 'label': 1})\n",
    "speech_df = pd.DataFrame(speech_data)\n",
    "\n",
    "print(f\"Handwriting samples: {len(hand_df)}\")\n",
    "print(f\"Speech samples: {len(speech_df)}\")\n",
    "\n",
    "# Split speech data (limiting factor)\n",
    "train_speech, temp_speech = train_test_split(speech_df, test_size=0.3, stratify=speech_df['label'], random_state=42)\n",
    "val_speech, test_speech = train_test_split(temp_speech, test_size=0.5, stratify=temp_speech['label'], random_state=42)\n",
    "\n",
    "print(f\"\\nTrain: {len(train_speech)}, Val: {len(val_speech)}, Test: {len(test_speech)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0391196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transform (matching handwriting model)\n",
    "image_transform = A.Compose([\n",
    "    A.Resize(224, 224),  # Match handwriting training\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Wav2Vec processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(hand_df, train_speech, processor, image_transform)\n",
    "val_dataset = MultimodalDataset(hand_df, val_speech, processor, image_transform)\n",
    "test_dataset = MultimodalDataset(hand_df, test_speech, processor, image_transform)\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"✓ Multimodal DataLoaders ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a7239",
   "metadata": {},
   "source": [
    "## 7. Load Pre-trained Models and Create Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64d2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load handwriting model\n",
    "handwriting_model = HandwritingModel(num_classes=2).to(device)\n",
    "hand_checkpoint = torch.load('handwriting_parkinsons_model_final.pth', map_location=device)\n",
    "handwriting_model.load_state_dict(hand_checkpoint['model_state_dict'])\n",
    "print(f\"✓ Handwriting model loaded (Acc: {hand_checkpoint['test_acc']:.4f})\")\n",
    "\n",
    "# Load speech model\n",
    "speech_model = SpeechModel(num_classes=2).to(device)\n",
    "speech_checkpoint = torch.load('speech_parkinsons_model_final.pth', map_location=device)\n",
    "speech_model.load_state_dict(speech_checkpoint['model_state_dict'])\n",
    "print(f\"✓ Speech model loaded (Acc: {speech_checkpoint['test_acc']:.4f})\")\n",
    "\n",
    "# Create fusion model\n",
    "fusion_model = MultimodalFusionModel(handwriting_model, speech_model, num_classes=2).to(device)\n",
    "print(f\"\\n✓ Fusion model created!\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in fusion_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48647f8f",
   "metadata": {},
   "source": [
    "## 8. Training Functions for Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b39e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fusion_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training Fusion')\n",
    "    for batch in pbar:\n",
    "        images = batch['image'].to(device)\n",
    "        audios = batch['audio'].to(device)\n",
    "        acoustic_feats = batch['acoustic_features'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, aux_outputs = model(images, audios, acoustic_feats, mode='fusion')\n",
    "        \n",
    "        # Main loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Auxiliary losses for individual modality predictions\n",
    "        aux_loss = 0.3 * (criterion(aux_outputs['hand_pred'], labels) + \n",
    "                          criterion(aux_outputs['speech_pred'], labels) +\n",
    "                          criterion(aux_outputs['fusion_pred'], labels))\n",
    "        \n",
    "        total_loss = loss + aux_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        pbar.set_postfix({'loss': running_loss / (pbar.n + 1)})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_fusion_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validating Fusion')\n",
    "        for batch in pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            audios = batch['audio'].to(device)\n",
    "            acoustic_feats = batch['acoustic_features'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs, _ = model(images, audios, acoustic_feats, mode='fusion')\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': running_loss / (pbar.n + 1)})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    return epoch_loss, epoch_acc, all_labels, all_preds, all_probs\n",
    "\n",
    "print(\"Training functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e663b4",
   "metadata": {},
   "source": [
    "## 9. Train Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c558740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup with improved parameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, fusion_model.parameters()), \n",
    "                              lr=5e-5, weight_decay=5e-4)  # Lower LR, higher weight decay\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_patience = 15\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "best_val_acc = 0.0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MULTIMODAL FUSION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    train_loss, train_acc = train_fusion_epoch(fusion_model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, _, _, _ = validate_fusion_epoch(fusion_model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save based on validation loss for better generalization\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'model_state_dict': fusion_model.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'epoch': epoch\n",
    "        }, 'best_fusion_model.pth')\n",
    "        print(f\"✓ Fusion model saved with val_acc: {val_acc:.4f}, val_loss: {val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{early_stopping_patience}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08663747",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_fusion_model.pth')\n",
    "fusion_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Test evaluation\n",
    "test_loss, test_acc, y_true, y_pred, y_probs = validate_fusion_epoch(fusion_model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MULTIMODAL FUSION MODEL - FINAL TEST RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Healthy', 'Parkinson'], digits=4))\n",
    "\n",
    "# Metrics\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_probs)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PERFORMANCE METRICS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"Specificity: {confusion_matrix(y_true, y_pred)[0,0]/(confusion_matrix(y_true, y_pred)[0,0]+confusion_matrix(y_true, y_pred)[0,1]):.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "# Compare with individual models\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MODEL COMPARISON\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Handwriting Model Accuracy: {hand_checkpoint['test_acc']:.4f}\")\n",
    "print(f\"Speech Model Accuracy: {speech_checkpoint['test_acc']:.4f}\")\n",
    "print(f\"Fusion Model Accuracy: {test_acc:.4f}\")\n",
    "print(f\"\\nImprovement over best individual: {(test_acc - max(hand_checkpoint['test_acc'], speech_checkpoint['test_acc']))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fbe484",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Training history\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(history['train_acc'], label='Train Acc', marker='o', linewidth=2)\n",
    "ax1.plot(history['val_acc'], label='Val Acc', marker='s', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Fusion Model Training History', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn', xticklabels=['Healthy', 'Parkinson'],\n",
    "            yticklabels=['Healthy', 'Parkinson'], ax=ax2, cbar_kws={'label': 'Count'})\n",
    "ax2.set_xlabel('Predicted', fontsize=11)\n",
    "ax2.set_ylabel('Actual', fontsize=11)\n",
    "ax2.set_title(f'Confusion Matrix\\nAcc: {test_acc:.4f}', fontsize=12, fontweight='bold')\n",
    "\n",
    "# ROC Curve\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "ax3.plot(fpr, tpr, label=f'Fusion (AUC={auc:.4f})', linewidth=3, color='darkgreen')\n",
    "ax3.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=2)\n",
    "ax3.set_xlabel('False Positive Rate', fontsize=11)\n",
    "ax3.set_ylabel('True Positive Rate', fontsize=11)\n",
    "ax3.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Model Comparison\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "models = ['Handwriting', 'Speech', 'Fusion']\n",
    "accuracies = [hand_checkpoint['test_acc'], speech_checkpoint['test_acc'], test_acc]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = ax4.bar(models, accuracies, color=colors, edgecolor='black', linewidth=2)\n",
    "ax4.set_ylabel('Accuracy', fontsize=11)\n",
    "ax4.set_title('Model Comparison', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylim([0, 1])\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{accuracies[i]:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "ax4.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Metrics comparison\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "metrics_values = [test_acc, precision, recall, f1, auc]\n",
    "x_pos = np.arange(len(metrics_names))\n",
    "bars = ax5.barh(x_pos, metrics_values, color='#16a085', edgecolor='black', linewidth=2)\n",
    "ax5.set_yticks(x_pos)\n",
    "ax5.set_yticklabels(metrics_names, fontsize=11)\n",
    "ax5.set_xlabel('Score', fontsize=11)\n",
    "ax5.set_title('Comprehensive Performance Metrics', fontsize=12, fontweight='bold')\n",
    "ax5.set_xlim([0, 1])\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax5.text(width + 0.02, bar.get_y() + bar.get_height()/2.,\n",
    "             f'{metrics_values[i]:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "ax5.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Multimodal Fusion Model - Complete Analysis', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.savefig('fusion_model_complete_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Comprehensive visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ac0e9",
   "metadata": {},
   "source": [
    "## 12. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bfa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete fusion model\n",
    "torch.save({\n",
    "    'fusion_model_state_dict': fusion_model.state_dict(),\n",
    "    'handwriting_model_state_dict': handwriting_model.state_dict(),\n",
    "    'speech_model_state_dict': speech_model.state_dict(),\n",
    "    'test_metrics': {\n",
    "        'accuracy': test_acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc_roc': auc\n",
    "    },\n",
    "    'individual_models': {\n",
    "        'handwriting_acc': hand_checkpoint['test_acc'],\n",
    "        'speech_acc': speech_checkpoint['test_acc']\n",
    "    }\n",
    "}, 'multimodal_fusion_parkinsons_final.pth')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ COMPLETE MULTIMODAL SYSTEM SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"1. multimodal_fusion_parkinsons_final.pth (Complete fusion model)\")\n",
    "print(\"2. fusion_model_complete_analysis.png (Visualization)\")\n",
    "print(\"\\nFinal Performance Summary:\")\n",
    "print(f\"  • Fusion Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  • AUC-ROC: {auc:.4f}\")\n",
    "print(f\"  • F1-Score: {f1:.4f}\")\n",
    "print(f\"  • Precision: {precision:.4f}\")\n",
    "print(f\"  • Recall: {recall:.4f}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
