{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504d05a0",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets\n",
    "!pip install -q librosa soundfile audiomentations\n",
    "!pip install -q praat-parselmouth pydub\n",
    "!pip install -q scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Set seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e61f0",
   "metadata": {},
   "source": [
    "## 2. Clone Dataset from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/Tvenkatathanuj/SDP.git\n",
    "\n",
    "# Set paths\n",
    "HEALTHY_SPEECH_PATH = '/content/SDP/speech dataset/HC_AH/HC_AH'\n",
    "PARKINSON_SPEECH_PATH = '/content/SDP/speech dataset/PD_AH/PD_AH'\n",
    "\n",
    "print(f\"Healthy speech samples: {len(os.listdir(HEALTHY_SPEECH_PATH))}\")\n",
    "print(f\"Parkinson speech samples: {len(os.listdir(PARKINSON_SPEECH_PATH))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb3980",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac483400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_acoustic_features(audio_path, sr=16000):\n",
    "    \"\"\"Extract traditional acoustic features for Parkinson's detection\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=sr)\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 1. MFCCs (40 coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "        features['mfcc_mean'] = np.mean(mfccs, axis=1)\n",
    "        features['mfcc_std'] = np.std(mfccs, axis=1)\n",
    "        \n",
    "        # 2. Chroma features (12 bins)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=12)\n",
    "        features['chroma_mean'] = np.mean(chroma, axis=1)\n",
    "        features['chroma_std'] = np.std(chroma, axis=1)\n",
    "        \n",
    "        # 3. Spectral features\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        \n",
    "        features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
    "        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
    "        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
    "        \n",
    "        # 4. Zero crossing rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        features['zcr_mean'] = np.mean(zcr)\n",
    "        \n",
    "        # 5. Jitter and Shimmer using Parselmouth\n",
    "        try:\n",
    "            sound = parselmouth.Sound(audio_path)\n",
    "            pitch = call(sound, \"To Pitch\", 0.0, 75, 600)\n",
    "            point_process = call(sound, \"To PointProcess (periodic, cc)\", 75, 600)\n",
    "            \n",
    "            jitter = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "            shimmer = call([sound, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "            \n",
    "            features['jitter'] = jitter\n",
    "            features['shimmer'] = shimmer\n",
    "        except:\n",
    "            features['jitter'] = 0\n",
    "            features['shimmer'] = 0\n",
    "        \n",
    "        # Flatten all features\n",
    "        feature_vector = np.concatenate([\n",
    "            features['mfcc_mean'], features['mfcc_std'],\n",
    "            features['chroma_mean'], features['chroma_std'],\n",
    "            [features['spectral_centroid_mean'], features['spectral_rolloff_mean'],\n",
    "             features['spectral_bandwidth_mean'], features['zcr_mean'],\n",
    "             features['jitter'], features['shimmer']]\n",
    "        ])\n",
    "        \n",
    "        return feature_vector\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Feature extraction functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb7d50",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3079d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "data = []\n",
    "\n",
    "for audio_file in os.listdir(HEALTHY_SPEECH_PATH):\n",
    "    if audio_file.endswith('.wav'):\n",
    "        data.append({\n",
    "            'audio_path': os.path.join(HEALTHY_SPEECH_PATH, audio_file),\n",
    "            'label': 0,  # Healthy\n",
    "            'class_name': 'Healthy'\n",
    "        })\n",
    "\n",
    "for audio_file in os.listdir(PARKINSON_SPEECH_PATH):\n",
    "    if audio_file.endswith('.wav'):\n",
    "        data.append({\n",
    "            'audio_path': os.path.join(PARKINSON_SPEECH_PATH, audio_file),\n",
    "            'label': 1,  # Parkinson\n",
    "            'class_name': 'Parkinson'\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nClass distribution:\\n{df['class_name'].value_counts()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9dcd0",
   "metadata": {},
   "source": [
    "## 5. Audio Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3897f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample audio files\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 12))\n",
    "fig.suptitle('Speech Signal Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Healthy sample\n",
    "healthy_sample = df[df['label'] == 0].sample(1).iloc[0]\n",
    "y_h, sr_h = librosa.load(healthy_sample['audio_path'], sr=16000)\n",
    "\n",
    "# Waveform\n",
    "axes[0, 0].plot(np.linspace(0, len(y_h)/sr_h, len(y_h)), y_h)\n",
    "axes[0, 0].set_title('Healthy - Waveform', color='green', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('Amplitude')\n",
    "\n",
    "# Spectrogram\n",
    "D_h = librosa.amplitude_to_db(np.abs(librosa.stft(y_h)), ref=np.max)\n",
    "librosa.display.specshow(D_h, sr=sr_h, x_axis='time', y_axis='hz', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Healthy - Spectrogram', color='green', fontweight='bold')\n",
    "\n",
    "# MFCC\n",
    "mfcc_h = librosa.feature.mfcc(y=y_h, sr=sr_h, n_mfcc=40)\n",
    "librosa.display.specshow(mfcc_h, sr=sr_h, x_axis='time', ax=axes[2, 0])\n",
    "axes[2, 0].set_title('Healthy - MFCC', color='green', fontweight='bold')\n",
    "\n",
    "# Mel Spectrogram\n",
    "mel_h = librosa.feature.melspectrogram(y=y_h, sr=sr_h)\n",
    "mel_h_db = librosa.power_to_db(mel_h, ref=np.max)\n",
    "librosa.display.specshow(mel_h_db, sr=sr_h, x_axis='time', y_axis='mel', ax=axes[3, 0])\n",
    "axes[3, 0].set_title('Healthy - Mel Spectrogram', color='green', fontweight='bold')\n",
    "\n",
    "# Parkinson sample\n",
    "parkinson_sample = df[df['label'] == 1].sample(1).iloc[0]\n",
    "y_p, sr_p = librosa.load(parkinson_sample['audio_path'], sr=16000)\n",
    "\n",
    "# Waveform\n",
    "axes[0, 1].plot(np.linspace(0, len(y_p)/sr_p, len(y_p)), y_p, color='red')\n",
    "axes[0, 1].set_title('Parkinson - Waveform', color='red', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time (s)')\n",
    "axes[0, 1].set_ylabel('Amplitude')\n",
    "\n",
    "# Spectrogram\n",
    "D_p = librosa.amplitude_to_db(np.abs(librosa.stft(y_p)), ref=np.max)\n",
    "librosa.display.specshow(D_p, sr=sr_p, x_axis='time', y_axis='hz', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Parkinson - Spectrogram', color='red', fontweight='bold')\n",
    "\n",
    "# MFCC\n",
    "mfcc_p = librosa.feature.mfcc(y=y_p, sr=sr_p, n_mfcc=40)\n",
    "librosa.display.specshow(mfcc_p, sr=sr_p, x_axis='time', ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Parkinson - MFCC', color='red', fontweight='bold')\n",
    "\n",
    "# Mel Spectrogram\n",
    "mel_p = librosa.feature.melspectrogram(y=y_p, sr=sr_p)\n",
    "mel_p_db = librosa.power_to_db(mel_p, ref=np.max)\n",
    "librosa.display.specshow(mel_p_db, sr=sr_p, x_axis='time', y_axis='mel', ax=axes[3, 1])\n",
    "axes[3, 1].set_title('Parkinson - Mel Spectrogram', color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('speech_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9218a8b",
   "metadata": {},
   "source": [
    "## 6. Custom Dataset with Audio Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b54659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, max_length=16000*5, augment=False):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Audio augmentation (more aggressive for small dataset)\n",
    "        if augment:\n",
    "            self.augmentation = Compose([\n",
    "                AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.025, p=0.7),\n",
    "                TimeStretch(min_rate=0.7, max_rate=1.3, p=0.7),\n",
    "                PitchShift(min_semitones=-5, max_semitones=5, p=0.7),\n",
    "            ])\n",
    "        else:\n",
    "            self.augmentation = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.df.iloc[idx]['audio_path']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        \n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=16000)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.augment and self.augmentation:\n",
    "            y = self.augmentation(samples=y, sample_rate=sr)\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(y) < self.max_length:\n",
    "            y = np.pad(y, (0, self.max_length - len(y)), mode='constant')\n",
    "        else:\n",
    "            y = y[:self.max_length]\n",
    "        \n",
    "        # Process with Wav2Vec2 processor\n",
    "        inputs = self.processor(y, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Extract acoustic features\n",
    "        acoustic_features = extract_acoustic_features(audio_path)\n",
    "        if acoustic_features is None:\n",
    "            acoustic_features = np.zeros(110)  # Default size: 40+40+12+12+4+2=110\n",
    "        \n",
    "        return {\n",
    "            'input_values': inputs.input_values.squeeze(0),\n",
    "            'acoustic_features': torch.FloatTensor(acoustic_features),\n",
    "            'label': torch.LongTensor([label])[0]\n",
    "        }\n",
    "\n",
    "print(\"Dataset class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdfc4c7",
   "metadata": {},
   "source": [
    "## 7. Novel Model: Wav2Vec 2.0 + BiLSTM + Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a06faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Self-Attention for temporal feature refinement\"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attended, _ = self.attention(x, x, x)\n",
    "        x = self.norm(x + self.dropout(attended))\n",
    "        return x\n",
    "\n",
    "class SpeechParkinsonsModel(nn.Module):\n",
    "    \"\"\"Hybrid Model: Wav2Vec 2.0 + BiLSTM + Multi-Head Attention + Acoustic Features\"\"\"\n",
    "    def __init__(self, num_classes=2, acoustic_feature_dim=110, lstm_hidden=256, num_heads=8):\n",
    "        super(SpeechParkinsonsModel, self).__init__()\n",
    "        \n",
    "        # Wav2Vec 2.0 backbone (frozen initially)\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        wav2vec_dim = 768  # Wav2Vec2-base output dimension\n",
    "        \n",
    "        # Freeze Wav2Vec initially\n",
    "        for param in self.wav2vec.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Bidirectional LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=wav2vec_dim,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        lstm_output_dim = lstm_hidden * 2  # Bidirectional\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.attention = MultiHeadAttention(lstm_output_dim, num_heads=num_heads)\n",
    "        \n",
    "        # Acoustic feature branch (stronger)\n",
    "        self.acoustic_branch = nn.Sequential(\n",
    "            nn.Linear(acoustic_feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Fusion and classification\n",
    "        fusion_dim = lstm_output_dim + 128\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Feature extractor for fusion model\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def unfreeze_wav2vec(self):\n",
    "        \"\"\"Unfreeze Wav2Vec for fine-tuning\"\"\"\n",
    "        for param in self.wav2vec.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def forward(self, input_values, acoustic_features, return_features=False):\n",
    "        # Wav2Vec 2.0 encoding\n",
    "        wav2vec_output = self.wav2vec(input_values).last_hidden_state\n",
    "        \n",
    "        # BiLSTM\n",
    "        lstm_out, _ = self.lstm(wav2vec_output)\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        attended = self.attention(lstm_out)\n",
    "        \n",
    "        # Global pooling\n",
    "        pooled = torch.mean(attended, dim=1)\n",
    "        \n",
    "        # Acoustic features\n",
    "        acoustic_out = self.acoustic_branch(acoustic_features)\n",
    "        \n",
    "        # Fusion\n",
    "        fused = torch.cat([pooled, acoustic_out], dim=1)\n",
    "        \n",
    "        if return_features:\n",
    "            # Return features for fusion model\n",
    "            features = self.feature_extractor(fused)\n",
    "            return features\n",
    "        \n",
    "        # Classification\n",
    "\n",
    "        out = self.classifier(fused)print(\"Speech model architecture defined!\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e58486",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        acoustic_features = batch['acoustic_features'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_values, acoustic_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': running_loss / (pbar.n + 1)})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for batch in pbar:\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            acoustic_features = batch['acoustic_features'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_values, acoustic_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': running_loss / (pbar.n + 1)})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_labels, all_preds, all_probs\n",
    "\n",
    "print(\"Training functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29346176",
   "metadata": {},
   "source": [
    "## 9. Data Preparation and Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5117f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (with oversampling for small dataset)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Initialize Wav2Vec2 processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SpeechDataset(train_df, processor, augment=True)\n",
    "val_dataset = SpeechDataset(val_df, processor, augment=False)\n",
    "test_dataset = SpeechDataset(test_df, processor, augment=False)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 4  # Small due to limited data\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"DataLoaders created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e753a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = SpeechParkinsonsModel(num_classes=2).to(device)\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                     classes=np.unique(train_df['label']), \n",
    "                                     y=train_df['label'].values)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "print(\"Ready to train!\")\n",
    "\n",
    "# Loss with class weighting and optimizerprint(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-3)  # Higher LR for small datasetscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf59ad",
   "metadata": {},
   "source": [
    "## 10. Training (Two-Stage: Frozen then Fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e496ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Train with frozen Wav2Vec\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 1: Training with Frozen Wav2Vec2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "NUM_EPOCHS_STAGE1 = 50  # Increased from 30\n",
    "best_val_acc = 0.0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_STAGE1):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS_STAGE1}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, _, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'speech_model_stage1.pth')\n",
    "        print(f\"✓ Model saved with val_acc: {val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nStage 1 Best Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eded311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Fine-tune entire model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2: Fine-tuning Wav2Vec2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load best model from stage 1\n",
    "model.load_state_dict(torch.load('speech_model_stage1.pth'))\n",
    "\n",
    "# Unfreeze Wav2Vec\n",
    "model.unfreeze_wav2vec()\n",
    "\n",
    "# Lower learning rate for fine-tuning\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=1e-3)  # Much lower LR\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "NUM_EPOCHS_STAGE2 = 30  # Increased from 20\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_STAGE2):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS_STAGE2}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, _, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_speech_model.pth')\n",
    "        print(f\"✓ Model saved with val_acc: {val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nStage 2 Best Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b7d9a",
   "metadata": {},
   "source": [
    "## 11. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_speech_model.pth'))\n",
    "\n",
    "# Test evaluation\n",
    "test_loss, test_acc, y_true, y_pred, y_probs = validate_epoch(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TEST SET PERFORMANCE\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Healthy', 'Parkinson'], digits=4))\n",
    "\n",
    "# Additional metrics\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_probs)\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708cae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Training history\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0, 0].axvline(x=30, color='red', linestyle='--', label='Fine-tuning start')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training History - Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "axes[0, 1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "axes[0, 1].axvline(x=30, color='red', linestyle='--', label='Fine-tuning start')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Training History - Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Healthy', 'Parkinson'],\n",
    "            yticklabels=['Healthy', 'Parkinson'], ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "axes[1, 0].set_title(f'Confusion Matrix\\nAccuracy: {test_acc:.4f}')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "axes[1, 1].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})', linewidth=2)\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[1, 1].set_xlabel('False Positive Rate')\n",
    "axes[1, 1].set_ylabel('True Positive Rate')\n",
    "axes[1, 1].set_title('ROC Curve')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('speech_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839dc3d",
   "metadata": {},
   "source": [
    "## 12. Save Model for Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'processor': processor,\n",
    "    'test_acc': test_acc,\n",
    "    'test_auc': auc,\n",
    "}, 'speech_parkinsons_model_final.pth')\n",
    "\n",
    "print(\"✓ Model saved for fusion: speech_parkinsons_model_final.pth\")\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
